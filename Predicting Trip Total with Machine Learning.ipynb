{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "156ec8ef-e248-492d-85c5-e1375752396e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#PART 3: Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9391a15-e974-4465-b310-9e5cb6f2d56e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import Imputer\n",
    "from pyspark.ml.regression import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59c6ead2-8e76-4673-b4af-a39eefe63c77",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .appName('at2_part3_ml') \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "398b663c-a59c-404a-9aa3-77ca719d3463",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "parquet_output_path = \"/FileStore/my_dataframe/final_df\"\n",
    "df_final = spark.read.parquet(parquet_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d5433f6-7a0c-473e-8d67-13b2f9b9191f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_final.createOrReplaceTempView(\"green_yellow_taxi_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae368fd4-5b4f-4aa4-a6ce-2847dbfed092",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[5]: ['lpep_pickup_datetime',\n 'lpep_dropoff_datetime',\n 'RatecodeID',\n 'PULocationID',\n 'DOLocationID',\n 'passenger_count',\n 'fare_amount',\n 'extra',\n 'mta_tax',\n 'tip_amount',\n 'tolls_amount',\n 'ehail_fee',\n 'improvement_surcharge',\n 'total_amount',\n 'payment_type',\n 'trip_type',\n 'congestion_surcharge',\n 'trip_distance_km',\n 'trip_duration_hours',\n 'speed_kmph',\n 'airport_fee',\n 'taxi_color',\n 'pickup_borough',\n 'pickup_zone',\n 'dropoff_borough',\n 'dropoff_zone']"
     ]
    }
   ],
   "source": [
    "df_final.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13765a63-0439-46a6-bc08-431425ec5154",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13873f46-e924-4975-9f39-f0dedf4f70e9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Features selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "773e5494-e60d-4062-8a2e-0a5a114ced12",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "col_list = ['passenger_count','lpep_pickup_datetime','trip_distance_km','trip_duration_hours','tip_amount','total_amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "395a02c6-9d0d-4ca2-bc52-1c11990b143b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_cleaned = df_final.select(col_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a81afbb-e6b0-4f7c-9e13-e872402b0243",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Raplace missing value with mean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "625d95b5-fb51-42f6-958c-5a488ba5a4f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "imputer = Imputer(\n",
    "    inputCols=['passenger_count'], \n",
    "    outputCols=['passenger_count_imputed'],  \n",
    "    strategy='mean'  \n",
    ")\n",
    "\n",
    "# Fit and transform the data\n",
    "df_cleaned= imputer.fit(df_cleaned).transform(df_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ae974da-b793-4a4b-9cf6-e7439572a63f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Split data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac41a6f9-121a-4f85-9fc4-dc634062164a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Filter the data for October, November, and December 2022 for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6af858b5-152d-4f04-9c95-23f661d55922",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test_set = df_cleaned.filter(\n",
    "    (year(df_cleaned.lpep_pickup_datetime) == 2022) &\n",
    "    (month(df_cleaned.lpep_pickup_datetime).isin([10, 11, 12]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b07054f5-c574-47b7-a1f0-f17f9772bc80",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_data = df_cleaned.subtract(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f9630f0-8558-4fc0-985f-a5dce6b14f22",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_data = train_data.drop(\"lpep_pickup_datetime\",\"passenger_count\")\n",
    "test_set = test_set.drop(\"lpep_pickup_datetime\",\"passenger_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da709f12-0665-4488-bfac-ad19c49ab42b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sample 0.0001 (0.01%) of the data for training\n",
    "sampled_train = train_data.sample(fraction=0.0001, seed=42)\n",
    "sampled_test = test_set.sample(fraction=0.0001, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95e9a807-4eec-409d-95f4-547e865c9b2c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Split the train_data into training (80%) and validation (20%) sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1fedad4-9d39-4d3b-93a1-b129bb142511",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_set, val_set = sampled_train.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f69c0c3-552d-48c1-8a8f-52a643426a29",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35bb0492-2115-4408-8570-a532cb03b615",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Trip_stat =\"\"\"\n",
    "SELECT\n",
    "    taxi_color AS TaxiColor,\n",
    "    CONCAT(pickup_zone, ' to ', dropoff_zone) AS Pickup_Dropoff,\n",
    "    CONCAT(LPAD(MONTH(lpep_pickup_datetime), 2, '0')) AS Month,\n",
    "    DATE_FORMAT(lpep_pickup_datetime, 'EEEE') AS DayOfWeek,\n",
    "    HOUR(lpep_pickup_datetime) AS HourOfDay,\n",
    "    COUNT(*) AS TotalTrips,\n",
    "    AVG(trip_distance_km) AS AvgDistance,\n",
    "    SUM(total_amount) / COUNT(*) AS AvgAmountPaid,\n",
    "    SUM(total_amount) AS TotalAmountPaid\n",
    "FROM\n",
    "    green_yellow_taxi_df\n",
    "WHERE trip_distance_km > 0\n",
    "GROUP BY\n",
    "    TaxiColor, Pickup_Dropoff, Month, DayOfWeek, HourOfDay\n",
    "ORDER BY \n",
    "    TaxiColor, Pickup_Dropoff, Month, DayOfWeek, HourOfDay\n",
    "\"\"\"\n",
    "\n",
    "TripSummary = spark.sql(Trip_stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5567ee7b-2368-4cae-8df0-fc469996c1ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Model RMSE: 3334.553055253188\n"
     ]
    }
   ],
   "source": [
    "y_base = TripSummary.selectExpr(\"AvgAmountPaid\").first()[0]\n",
    "\n",
    "# Create a new DataFrame with predicted values (y_base - 'TotalAmountPaid')\n",
    "baseline_predictions = TripSummary.withColumn(\"prediction\", y_base - col(\"TotalAmountPaid\"))\n",
    "\n",
    "# Evaluate the baseline model using an appropriate evaluation metric\n",
    "evaluator = RegressionEvaluator(labelCol=\"TotalAmountPaid\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(baseline_predictions)\n",
    "\n",
    "# Print the RMSE of the baseline model\n",
    "print(f\"Baseline Model RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39d92cc7-66e5-4248-b1a3-a80c3c5de967",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "y_base = sampled_train.selectExpr(\"avg(total_amount) as prediction\").first()[0]\n",
    "\n",
    "# Create a new DataFrame with predicted values (y_base - 'TotalAmountPaid')\n",
    "baseline_predictions = sampled_train.withColumn(\"prediction\", y_base - col(\"total_amount\"))\n",
    "\n",
    "# Evaluate the baseline model using an appropriate evaluation metric\n",
    "evaluator = RegressionEvaluator(labelCol=\"total_amount\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(baseline_predictions)\n",
    "\n",
    "# Print the RMSE of the baseline model\n",
    "print(f\"Baseline Model RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3d447fa-9ccf-4dae-a222-382ffb266495",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "y_mean = sampled_train.selectExpr(\"avg(total_amount) as prediction\").first().prediction\n",
    "\n",
    "# Create a new DataFrame with predicted values\n",
    "baseline = sampled_train.select(\"total_amount\").withColumn(\"basline_pred\",lit(y_mean))\n",
    "\n",
    "# Evaluate the baseline model\n",
    "evaluator = RegressionEvaluator(labelCol=\"total_amount\", predictionCol=\"basline_pred\", metricName=\"rmse\")\n",
    "basline_rmse = evaluator.evaluate(baseline)\n",
    "\n",
    "# Print the RMSE of the baseline model\n",
    "print(f\"Baseline Model RMSE: {basline_rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2b82126-d4f8-4c9d-a80e-8e363c566b9c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2207ddd4-b12f-43bd-bbb2-d7d6cba83b3e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "feature_columns = [\n",
    "    'passenger_count_imputed',\n",
    "    'trip_distance_km',\n",
    "    'trip_duration_hours',\n",
    "    'tip_amount'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "208d3444-549c-4521-b505-4c10dfc05cfb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "vector_assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a8aab7b-a82d-4650-a538-fa915c471eed",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#####Fitting model on Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f082d553-195e-4c3c-b072-38dc6db4c282",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"total_amount\")\n",
    "pipeline_lr = Pipeline(stages=[vector_assembler, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c5b9a89-fecd-4405-8ec4-a41fc1a35e0a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_lr = pipeline_lr.fit(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c4f900c-36ba-43ce-a58d-ffca30f92782",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###### Save the model using Spark's built-in serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28f5c635-f0bc-4192-958b-63d67e44bed5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Directory path\n",
    "directory_path = '/dbfs/mnt/at2-bde/'\n",
    "\n",
    "# Create the directory \n",
    "if not os.path.exists(directory_path):\n",
    "    os.makedirs(directory_path)\n",
    "\n",
    "# Specify the complete directory path where to save the model\n",
    "model_path = \"/dbfs/mnt/at2-bde/model_lr\"\n",
    "model_lr.write().overwrite().save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0f61c31-7ee8-451c-95ad-abb12042020e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.pipeline import PipelineModel\n",
    "\n",
    "# Load the model\n",
    "model_lr = PipelineModel.load('/dbfs/mnt/at2-bde/model_lr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9958790-36d6-45cc-bd27-b364d4b42904",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#####Train model on Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7917c1d9-360f-4d83-93f4-ba858124bba4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression RMSE: 3.3556476540654008\n"
     ]
    }
   ],
   "source": [
    "lr_pred = model_lr.transform(val_set)\n",
    "\n",
    "# Evaluate the model on the val data\n",
    "evaluator_lreg = RegressionEvaluator(labelCol=\"total_amount\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "lr_rmse = evaluator_lreg.evaluate(lr_pred)\n",
    "\n",
    "print(\"Linear Regression RMSE:\", lr_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24211e10-b94b-4e66-89c2-3dd2af119030",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb1c4183-27c7-4d6b-be1a-b1cad6d0c4f7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Fiting the model on Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "224f9eca-e707-4ad9-998c-19ffe02ea823",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dt = DecisionTreeRegressor(featuresCol='features', labelCol='total_amount', maxDepth=5)\n",
    "pipeline_dt = Pipeline(stages=[vector_assembler, dt])\n",
    "\n",
    "model_dt = pipeline_dt.fit(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f4a49dc-f761-4569-bd71-3e999903d693",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Directory path\n",
    "directory_path = '/dbfs/mnt/at2-bde/'\n",
    "\n",
    "# Create the directory \n",
    "if not os.path.exists(directory_path):\n",
    "    os.makedirs(directory_path)\n",
    "\n",
    "# Specify the complete directory path where to save the model\n",
    "model_path = \"/dbfs/mnt/at2-bde/model_dt\"\n",
    "model_dt.write().overwrite().save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d80feb7-22d9-4e30-8a60-8420768d7813",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.pipeline import PipelineModel\n",
    "\n",
    "# Load the model\n",
    "model_dt = PipelineModel.load('/dbfs/mnt/at2-bde/model_dt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "099250e1-b5ca-476d-941f-cf63a8aef426",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Fiting the model on Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0bf8799-2ef2-42bc-b3ec-a2c497182c72",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree RMSE: 4.194063080024141\n"
     ]
    }
   ],
   "source": [
    "dt_pred = model_dt.transform(val_set)\n",
    "\n",
    "evaluator_dt = RegressionEvaluator(labelCol='total_amount', predictionCol='prediction', metricName='rmse')\n",
    "dt_rmse = evaluator_dt.evaluate(dt_pred)\n",
    "\n",
    "print(f\"Decision Tree RMSE: {dt_rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29acf506-4647-4bee-a49b-17599c7207bd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Train the best model on Test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "320d0225-66dd-44fd-9491-98598a16848b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d14059a1-014b-4180-b158-dcf36b4e97b7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Fit the best model all test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46cf0210-d997-4a4c-895f-b5de75a86121",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression RMSE on Test set: 6.967146645795853\n"
     ]
    }
   ],
   "source": [
    "lr_pred_test = model_lr.transform(test_set)\n",
    "\n",
    "# Evaluate the model on the val data\n",
    "evaluator_lreg_test = RegressionEvaluator(labelCol=\"total_amount\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "lr_rmse_test = evaluator_lreg_test.evaluate(lr_pred_test)\n",
    "print(\"Linear Regression RMSE on Test set:\", lr_rmse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dbd0f42a-ff54-4bb4-87f8-464cdb93ca9a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Fit the best model with sampled test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52bf6bb7-1349-4c6c-8e62-de2be642c584",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression RMSE on Test set: 6.729204590213371\n"
     ]
    }
   ],
   "source": [
    "lr_pred_samtest = model_lr.transform(sampled_test)\n",
    "\n",
    "# Evaluate the model on the val data\n",
    "evaluator_lreg_samtest = RegressionEvaluator(labelCol=\"total_amount\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "lr_rmse_samtest = evaluator_lreg_samtest.evaluate(lr_pred_samtest)\n",
    "print(\"Linear Regression RMSE on Test set:\", lr_rmse_samtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "744eced1-ff0d-4651-9e42-2eec5128a134",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed2cd27b-1951-49f4-a3c9-f88b27be7784",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Fit the model all test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73c3f954-20c9-4599-ac5c-826ca31387b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree RMSE on Test set: 8.061201048524683\n"
     ]
    }
   ],
   "source": [
    "dt_pred_test = model_dt.transform(test_set)\n",
    "\n",
    "evaluator_dt_test = RegressionEvaluator(labelCol='total_amount', predictionCol='prediction', metricName='rmse')\n",
    "dt_rmse_test = evaluator_dt_test.evaluate(dt_pred_test)\n",
    "\n",
    "print(f\"Decision Tree RMSE on Test set: {dt_rmse_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63ad99ec-d110-44b9-ae85-9cabb0164136",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Fit the model sampled test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8deeba8d-1c0a-4fca-af6d-f3e1807c1bf1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree RMSE on Test set: 8.230829771750095\n"
     ]
    }
   ],
   "source": [
    "dt_pred_samtest = model_dt.transform(sampled_test)\n",
    "\n",
    "evaluator_dt_test = RegressionEvaluator(labelCol='total_amount', predictionCol='prediction', metricName='rmse')\n",
    "dt_rmse_test = evaluator_dt_test.evaluate(dt_pred_samtest)\n",
    "\n",
    "print(f\"Decision Tree RMSE on Test set: {dt_rmse_test}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Part3_Panalee",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
